{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faacc1a1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "19261d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Any\n",
    "from collections.abc import Iterable\n",
    "from abc import ABC, abstractmethod\n",
    "import numbers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f284f531",
   "metadata": {},
   "source": [
    "### Classe NameManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd00b34",
   "metadata": {},
   "source": [
    "A classe NameManager provê uma forma conveniente de dar nomes intuitivos para tensores que resultam de operações. A idéia é tornar mais fácil para o usuário das demais classes qual operação gerou qual tensor. Ela provê os seguintes métodos públicos: \n",
    "\n",
    "- reset(): reinicia o sistema de gestão de nomes.\n",
    "- new(<basename>: str): retorna um nome único a partir do nome de base passado como argumento. \n",
    "  \n",
    "Como indicado no exemplo abaixo da classe, a idéia geral é que uma sequência de operações é feita, os nomes dos tensores sejam os nomes das operações seguidos de um número. Se forem feitas 3 operações de soma e uma de multiplicação, seus tensores de saída terão os nomes \"add:0\", \"add:1\", \"add:2\" e \"prod:0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "162272a0",
   "metadata": {
    "tags": [
     "name_manager"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add:0\n",
      "in:0\n",
      "add:1\n",
      "add:2\n",
      "in:1\n",
      "prod:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NameManager:\n",
    "    _counts = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def reset():\n",
    "        NameManager._counts = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def _count(name):\n",
    "        if name not in NameManager._counts:\n",
    "            NameManager._counts[name] = 0\n",
    "        count = NameManager._counts[name]\n",
    "        return count\n",
    "\n",
    "    @staticmethod\n",
    "    def _inc_count(name):\n",
    "        assert name in NameManager._counts, f'Name {name} is not registered.'\n",
    "        NameManager._counts[name] += 1\n",
    "\n",
    "    @staticmethod\n",
    "    def new(name: str):\n",
    "        count = NameManager._count(name)\n",
    "        tensor_name = f\"{name}:{count}\"\n",
    "        NameManager._inc_count(name)\n",
    "        return tensor_name\n",
    "\n",
    "# exemplo de uso\n",
    "print(NameManager.new('add'))\n",
    "print(NameManager.new('in'))\n",
    "print(NameManager.new('add'))\n",
    "print(NameManager.new('add'))\n",
    "print(NameManager.new('in'))\n",
    "print(NameManager.new('prod'))\n",
    "\n",
    "NameManager.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69485a9",
   "metadata": {},
   "source": [
    "### Classe Tensor\n",
    "\n",
    "Deve ser criada uma classe `Tensor` representando um array multidimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "448496d7",
   "metadata": {
    "tags": [
     "tensor"
    ]
   },
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    \"\"\"Classe representando um array multidimensional.\n",
    "\n",
    "    Atributos:\n",
    "\n",
    "    - _arr  (privado): dados internos do tensor como\n",
    "        um array do numpy com 2 dimensões (ver Regras)\n",
    "\n",
    "    - _parents (privado): lista de tensores que foram\n",
    "        usados como argumento para a operação que gerou o\n",
    "        tensor. Será vazia se o tensor foi inicializado com\n",
    "        valores diretamente. Por exemplo, se o tensor foi\n",
    "        resultado da operação a + b entre os tensores a e b,\n",
    "        _parents = [a, b].\n",
    "\n",
    "    - requires_grad (público): indica se devem ser\n",
    "        calculados gradientes para o tensor ou não.\n",
    "\n",
    "    - grad (público): Tensor representando o gradiente.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    _tensor_counter = 0  # Contador global para nomes únicos\n",
    "\n",
    "    def __init__(self,\n",
    "                 # Dados do tensor. Além dos tipos listados,\n",
    "                 # arr também pode ser do tipo Tensor.\n",
    "                 arr: Union[np.ndarray, list, numbers.Number, Any],\n",
    "                 # Entradas da operacao que gerou o tensor.\n",
    "                 # Deve ser uma lista de itens do tipo Tensor.\n",
    "                 parents: list[Any] = None,\n",
    "                 # se o tensor requer o calculo de gradientes ou nao\n",
    "                 requires_grad: bool = True,\n",
    "                 # nome do tensor\n",
    "                 name: str = '',\n",
    "                 # referência para um objeto do tipo Operation (ou\n",
    "                 # subclasse) indicando qual operação gerou este\n",
    "                 # tensor. Este objeto também possui um método\n",
    "                 # para calcular a derivada da operação.\n",
    "                 operation=None):\n",
    "        \"\"\"Construtor\n",
    "\n",
    "        O construtor deve permitir a criacao de tensores das seguintes formas:\n",
    "\n",
    "            # a partir de escalares\n",
    "            x = Tensor(3)\n",
    "\n",
    "            # a partir de listas\n",
    "            x = Tensor([1,2,3])\n",
    "\n",
    "            # a partir de arrays\n",
    "            x = Tensor(np.array([1,2,3]))\n",
    "\n",
    "            # a partir de outros tensores (construtor de copia)\n",
    "            x = Tensor(Tensor(np.array([1,2,3])))\n",
    "\n",
    "        Para isto, as seguintes regras devem ser obedecidas:\n",
    "\n",
    "        - Se o argumento arr não for um array do numpy,\n",
    "            ele deve ser convertido em um. Defina o dtype do\n",
    "            array como float de forma a permitir que NÃO seja\n",
    "            necessário passar constantes float como Tensor(3.0),\n",
    "            mas possamos criar um tensor apenas com Tensor(3).\n",
    "\n",
    "        - O atributo _arr deve ser uma matriz, isto é,\n",
    "            ter 2 dimensões (ver Regras).\n",
    "\n",
    "        - Se o argumento arr for um Tensor, ele deve ser\n",
    "            copiado (cuidado com cópias por referência).\n",
    "\n",
    "        - Se arr for um array do numpy com 1 dimensão,\n",
    "            ele deve ser convertido em uma matriz coluna.\n",
    "\n",
    "        - Se arr for um array do numpy com dimensão maior\n",
    "            que 2, deve ser lançada uma exceção.\n",
    "\n",
    "        - Tensores que não foram produzidos como resultado\n",
    "            de uma operação não têm pais nem operação.\n",
    "            Os nomes destes tensores devem seguir o formato in:3.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Inicializar parents como lista vazia se None\n",
    "        if parents is None:\n",
    "            parents = []\n",
    "        \n",
    "        # Processar o input arr\n",
    "        if isinstance(arr, Tensor):\n",
    "            # Construtor de cópia - copia o array interno\n",
    "            self._arr = arr._arr.copy()\n",
    "        elif isinstance(arr, np.ndarray):\n",
    "            # Verificar dimensionalidade\n",
    "            if arr.ndim > 2:\n",
    "                raise ValueError(f\"Arrays com mais de 2 dimensões não são suportados. Recebido: {arr.ndim} dimensões\")\n",
    "            elif arr.ndim == 0:\n",
    "                # Escalar numpy -> matriz 1x1\n",
    "                self._arr = np.array([[arr.item()]], dtype=float)\n",
    "            elif arr.ndim == 1:\n",
    "                # Vetor -> matriz coluna\n",
    "                self._arr = arr.reshape(-1, 1).astype(float)\n",
    "            else:\n",
    "                # arr.ndim == 2, já é uma matriz\n",
    "                self._arr = arr.astype(float)\n",
    "        elif isinstance(arr, (list, tuple)):\n",
    "            # Converter lista/tupla para numpy array\n",
    "            np_arr = np.array(arr, dtype=float)\n",
    "            if np_arr.ndim > 2:\n",
    "                raise ValueError(f\"Listas com mais de 2 dimensões não são suportadas. Recebido: {np_arr.ndim} dimensões\")\n",
    "            elif np_arr.ndim == 0:\n",
    "                # Escalar -> matriz 1x1\n",
    "                self._arr = np.array([[np_arr.item()]], dtype=float)\n",
    "            elif np_arr.ndim == 1:\n",
    "                # Vetor -> matriz coluna\n",
    "                self._arr = np_arr.reshape(-1, 1)\n",
    "            else:\n",
    "                # np_arr.ndim == 2, já é uma matriz\n",
    "                self._arr = np_arr\n",
    "        elif isinstance(arr, numbers.Number):\n",
    "            # Escalar -> matriz 1x1\n",
    "            self._arr = np.array([[float(arr)]], dtype=float)\n",
    "        else:\n",
    "            # Tentar converter para numpy array\n",
    "            try:\n",
    "                np_arr = np.array(arr, dtype=float)\n",
    "                if np_arr.ndim > 2:\n",
    "                    raise ValueError(f\"Dados com mais de 2 dimensões não são suportados. Recebido: {np_arr.ndim} dimensões\")\n",
    "                elif np_arr.ndim == 0:\n",
    "                    self._arr = np.array([[np_arr.item()]], dtype=float)\n",
    "                elif np_arr.ndim == 1:\n",
    "                    self._arr = np_arr.reshape(-1, 1)\n",
    "                else:\n",
    "                    self._arr = np_arr\n",
    "            except:\n",
    "                raise TypeError(f\"Tipo não suportado para criação de Tensor: {type(arr)}\")\n",
    "        \n",
    "        # Garantir que _arr sempre tenha 2 dimensões\n",
    "        if self._arr.ndim != 2:\n",
    "            raise RuntimeError(f\"Erro interno: _arr deve ter 2 dimensões, mas tem {self._arr.ndim}\")\n",
    "        \n",
    "        # Configurar atributos\n",
    "        self._parents = parents\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = None\n",
    "        self._operation = operation\n",
    "        \n",
    "        # Configurar nome\n",
    "        if name:\n",
    "            self._name = name\n",
    "        elif not parents and operation is None:\n",
    "            # Tensor criado diretamente (não resultado de operação)\n",
    "            Tensor._tensor_counter += 1\n",
    "            self._name = f\"in:{Tensor._tensor_counter}\"\n",
    "        else:\n",
    "            # Tensor gerado por operação - nome será definido pela operação\n",
    "            self._name = name if name else \"op_result\"\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"Reinicia o gradiente com zero\"\"\"\n",
    "        if self.requires_grad:\n",
    "            self.grad = None\n",
    "\n",
    "    def numpy(self):\n",
    "        \"\"\"Retorna o array interno\"\"\"\n",
    "        return self._arr\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Permite visualizar os dados do tensor como string\"\"\"\n",
    "        return f\"Tensor({self._arr}, name={self._name}, shape={self._arr.shape})\"\n",
    "\n",
    "    def backward(self, my_grad=None):\n",
    "        \"\"\"Método usado tanto iniciar o processo de\n",
    "        diferenciação automática, quanto por um filho\n",
    "        para enviar o gradiente do pai. No primeiro\n",
    "        caso, o argumento my_grad não será passado.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Se requires_grad é False, não fazer nada\n",
    "        if not self.requires_grad:\n",
    "            return\n",
    "        \n",
    "        # Se my_grad não foi fornecido, este é o ponto de partida da backpropagation\n",
    "        if my_grad is None:\n",
    "            # Criar gradiente inicial (vetor de ones com mesma forma que o tensor)\n",
    "            my_grad = Tensor(np.ones_like(self._arr), requires_grad=False)\n",
    "        \n",
    "        # Verificar que o gradiente tem o mesmo shape que o tensor\n",
    "        assert my_grad.shape == self.shape, f\"Gradient shape {my_grad.shape} must match tensor shape {self.shape}\"\n",
    "        \n",
    "        # SOMAR gradientes (não substituir) - crucial para acumulação\n",
    "        if self.grad is None:\n",
    "            self.grad = Tensor(my_grad._arr.copy(), requires_grad=False)\n",
    "        else:\n",
    "            # Somar gradientes (para casos onde o mesmo tensor é usado múltiplas vezes)\n",
    "            assert self.grad.shape == my_grad.shape, f\"Existing gradient shape {self.grad.shape} must match new gradient shape {my_grad.shape}\"\n",
    "            self.grad = Tensor(self.grad._arr + my_grad._arr, requires_grad=False)\n",
    "        \n",
    "        # Se este tensor tem uma operação que o gerou, calcular gradientes dos pais\n",
    "        if self._operation is not None and self._parents:\n",
    "            # A operação calcula os gradientes usando o método grad()\n",
    "            parent_grads = self._operation.grad(my_grad, *self._parents)\n",
    "            \n",
    "            # Verificar que temos um gradiente para cada pai\n",
    "            assert len(parent_grads) == len(self._parents), f\"Number of gradients {len(parent_grads)} must match number of parents {len(self._parents)}\"\n",
    "            \n",
    "            # Propagar gradientes para os pais\n",
    "            for parent, parent_grad in zip(self._parents, parent_grads):\n",
    "                if parent.requires_grad:\n",
    "                    # Verificar que o gradiente tem o shape correto\n",
    "                    assert parent_grad.shape == parent.shape, f\"Parent gradient shape {parent_grad.shape} must match parent shape {parent.shape}\"\n",
    "                    parent.backward(parent_grad)\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        \"\"\"Retorna a forma do tensor\"\"\"\n",
    "        return self._arr.shape\n",
    "    \n",
    "    @property\n",
    "    def T(self):\n",
    "        \"\"\"Retorna a transposta do tensor\"\"\"\n",
    "        return Tensor(self._arr.T, requires_grad=self.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c612fc",
   "metadata": {},
   "source": [
    "### Interface de  Operações"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db44044",
   "metadata": {},
   "source": [
    "A classe abaixo define a interface que as operações devem implementar. Ela não precisa ser modificada, mas pode, caso queira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "28a19b73",
   "metadata": {
    "tags": [
     "op"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Op(ABC):\n",
    "    @abstractmethod\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando as entradas e\n",
    "            retorna o tensor resultado. O método deve\n",
    "            garantir que o atributo parents do tensor\n",
    "            de saída seja uma lista de tensores.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna os gradientes dos pais em como tensores.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "        - back_grad: Derivada parcial em relação à saída\n",
    "            da operação backpropagada pelo filho.\n",
    "\n",
    "        - args: variaveis de entrada da operacao (pais)\n",
    "            como tensores.\n",
    "\n",
    "        - O nome dos tensores de gradiente devem ter o\n",
    "            nome da operacao seguido de '_grad'.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_tensor(value):\n",
    "        \"\"\"Converte um valor em Tensor se não for um\"\"\"\n",
    "        if isinstance(value, Tensor):\n",
    "            return value\n",
    "        else:\n",
    "            # Converter escalar, lista, array, etc. em Tensor\n",
    "            return Tensor(value, requires_grad=False)\n",
    "\n",
    "    @staticmethod\n",
    "    # Função para broadcasting\n",
    "    def _broadcast_tensors(a, b):\n",
    "        \"\"\"\n",
    "        Implementa broadcasting entre dois tensors para compatibilidade de shapes.\n",
    "        Retorna os tensors com shapes compatíveis para operação element-wise.\n",
    "        \"\"\"\n",
    "        # Se shapes já são iguais, não precisa fazer nada\n",
    "        if a.shape == b.shape:\n",
    "            return a, b\n",
    "        \n",
    "        # Implementar regras básicas de broadcasting\n",
    "        shape_a = a.shape\n",
    "        shape_b = b.shape\n",
    "        \n",
    "        # Caso especial: se um é (1,1) e outro é (n,1), expandir o (1,1)\n",
    "        if shape_a == (1, 1) and shape_b[1] == 1:\n",
    "            # Expandir 'a' para o shape de 'b'\n",
    "            broadcasted_arr = np.broadcast_to(a._arr, shape_b)\n",
    "            new_a = Tensor(broadcasted_arr, requires_grad=a.requires_grad)\n",
    "            return new_a, b\n",
    "        elif shape_b == (1, 1) and shape_a[1] == 1:\n",
    "            # Expandir 'b' para o shape de 'a'\n",
    "            broadcasted_arr = np.broadcast_to(b._arr, shape_a)\n",
    "            new_b = Tensor(broadcasted_arr, requires_grad=b.requires_grad)\n",
    "            return a, new_b\n",
    "        \n",
    "        # Caso especial: se um é (1,n) e outro é (m,1), fazer broadcast para (m,n)\n",
    "        if shape_a[0] == 1 and shape_b[1] == 1:\n",
    "            target_shape = (shape_b[0], shape_a[1])\n",
    "            broadcasted_a = np.broadcast_to(a._arr, target_shape)\n",
    "            broadcasted_b = np.broadcast_to(b._arr, target_shape)\n",
    "            new_a = Tensor(broadcasted_a, requires_grad=a.requires_grad)\n",
    "            new_b = Tensor(broadcasted_b, requires_grad=b.requires_grad)\n",
    "            return new_a, new_b\n",
    "        elif shape_b[0] == 1 and shape_a[1] == 1:\n",
    "            target_shape = (shape_a[0], shape_b[1])\n",
    "            broadcasted_a = np.broadcast_to(a._arr, target_shape)\n",
    "            broadcasted_b = np.broadcast_to(b._arr, target_shape)\n",
    "            new_a = Tensor(broadcasted_a, requires_grad=a.requires_grad)\n",
    "            new_b = Tensor(broadcasted_b, requires_grad=b.requires_grad)\n",
    "            return new_a, new_b\n",
    "        \n",
    "        # Se chegou aqui, os shapes não são compatíveis para broadcasting\n",
    "        raise ValueError(f\"Shapes {shape_a} and {shape_b} are not compatible for broadcasting\")\n",
    "\n",
    "    @staticmethod\n",
    "    # Função para calcular gradientes com broadcasting\n",
    "    def _unbroadcast_gradient(grad, original_shape):\n",
    "        \"\"\"\n",
    "        Reduz um gradiente que foi expandido por broadcasting de volta ao shape original.\n",
    "        \"\"\"\n",
    "        if grad.shape == original_shape:\n",
    "            return grad\n",
    "        \n",
    "        grad_arr = grad._arr\n",
    "        \n",
    "        # Se o shape original era (1,1), somar todos os elementos\n",
    "        if original_shape == (1, 1):\n",
    "            summed = np.sum(grad_arr)\n",
    "            return Tensor([[summed]], requires_grad=False)\n",
    "        \n",
    "        # Se uma dimensão era 1, somar ao longo dessa dimensão\n",
    "        if original_shape[0] == 1 and grad.shape[0] > 1:\n",
    "            # Somar ao longo da primeira dimensão\n",
    "            summed = np.sum(grad_arr, axis=0, keepdims=True)\n",
    "            return Tensor(summed, requires_grad=False)\n",
    "        \n",
    "        if original_shape[1] == 1 and grad.shape[1] > 1:\n",
    "            # Somar ao longo da segunda dimensão\n",
    "            summed = np.sum(grad_arr, axis=1, keepdims=True)\n",
    "            return Tensor(summed, requires_grad=False)\n",
    "        \n",
    "        # Se chegou aqui, retornar o gradiente original\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89e386",
   "metadata": {},
   "source": [
    "### Implementação das Operações\n",
    "\n",
    "Operações devem herdar de `Op` e implementar os métodos `__call__` e `grad`.\n",
    "\n",
    "Pelo menos as seguintes operações devem ser implementadas:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aa4f7719",
   "metadata": {
    "tags": [
     "add"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Add(Op):\n",
    "    \"\"\"Add(a, b): a + b\"\"\"\n",
    "    \n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "        assert len(args) == 2, f\"Add operation requires exactly 2 arguments, got {len(args)}\"\n",
    "        \n",
    "        # Converter argumentos para Tensors se necessário\n",
    "        original_a, original_b = self._ensure_tensor(args[0]), self._ensure_tensor(args[1])\n",
    "        \n",
    "        # Fazer broadcasting dos tensors\n",
    "        a, b = self._broadcast_tensors(original_a, original_b)\n",
    "        \n",
    "        # Agora os shapes devem ser iguais\n",
    "        assert a.shape == b.shape, f\"After broadcasting, tensors must have same shape: {a.shape} vs {b.shape}\"\n",
    "        \n",
    "        # Realizar a operação\n",
    "        result_arr = a._arr + b._arr\n",
    "        \n",
    "        # Verificar que o resultado tem o shape esperado\n",
    "        assert result_arr.shape == a.shape, f\"Result shape {result_arr.shape} should match input shape {a.shape}\"\n",
    "        \n",
    "        # Criar tensor resultado com referências aos pais ORIGINAIS\n",
    "        parents = [original_a, original_b]\n",
    "        \n",
    "        result = Tensor(\n",
    "            result_arr,\n",
    "            parents=parents,\n",
    "            requires_grad=(original_a.requires_grad or original_b.requires_grad),\n",
    "            operation=self,\n",
    "            name=NameManager.new(\"add\")\n",
    "            \n",
    "        )\n",
    "        \n",
    "        # Guardar informações sobre broadcasting para usar no gradiente\n",
    "        result._broadcast_info = {\n",
    "            'original_shapes': (original_a.shape, original_b.shape),\n",
    "            'broadcasted_shapes': (a.shape, b.shape)\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "        assert len(args) == 2, f\"Add grad requires exactly 2 parent tensors, got {len(args)}\"\n",
    "        \n",
    "        original_a, original_b = args\n",
    "        \n",
    "        # Para soma, o gradiente é o mesmo para ambos os operandos\n",
    "        grad_a = Tensor(back_grad._arr.copy(), requires_grad=False, name=\"Add_grad\")\n",
    "        grad_b = Tensor(back_grad._arr.copy(), requires_grad=False, name=\"Add_grad\")\n",
    "        \n",
    "        # Fazer unbroadcast dos gradientes para os shapes originais\n",
    "        grad_a = self._unbroadcast_gradient(grad_a, original_a.shape)\n",
    "        grad_b = self._unbroadcast_gradient(grad_b, original_b.shape)\n",
    "        \n",
    "        # Verificar shapes dos gradientes\n",
    "        assert grad_a.shape == original_a.shape, f\"Gradient shape {grad_a.shape} must match parent shape {original_a.shape}\"\n",
    "        assert grad_b.shape == original_b.shape, f\"Gradient shape {grad_b.shape} must match parent shape {original_b.shape}\"\n",
    "        \n",
    "        return [grad_a, grad_b]\n",
    "\n",
    "# Instancia a operação\n",
    "add = Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "05cb44e6",
   "metadata": {
    "tags": [
     "sub"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sub(Op):\n",
    "    \"\"\"Sub(a, b): a - b\"\"\"\n",
    "    \n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a subtração usando os argumentos dados em args\"\"\"\n",
    "        assert len(args) == 2, f\"Sub operation requires exactly 2 arguments, got {len(args)}\"\n",
    "        \n",
    "        # Converter argumentos para Tensors se necessário\n",
    "        original_a, original_b = self._ensure_tensor(args[0]), self._ensure_tensor(args[1])\n",
    "        \n",
    "        # Fazer broadcasting dos tensors\n",
    "        a, b = self._broadcast_tensors(original_a, original_b)\n",
    "        \n",
    "        # Agora os shapes devem ser iguais\n",
    "        assert a.shape == b.shape, f\"After broadcasting, tensors must have same shape: {a.shape} vs {b.shape}\"\n",
    "        \n",
    "        # Realizar a operação\n",
    "        result_arr = a._arr - b._arr\n",
    "        \n",
    "        # Verificar que o resultado tem o shape esperado\n",
    "        assert result_arr.shape == a.shape, f\"Result shape {result_arr.shape} should match input shape {a.shape}\"\n",
    "        \n",
    "        # Criar tensor resultado com referências aos pais ORIGINAIS\n",
    "        parents = [original_a, original_b]\n",
    "        \n",
    "        result = Tensor(\n",
    "            result_arr,\n",
    "            parents=parents,\n",
    "            requires_grad=(original_a.requires_grad or original_b.requires_grad),\n",
    "            operation=self,\n",
    "            name=NameManager.new(\"sub\")\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais\"\"\"\n",
    "        assert len(args) == 2, f\"Sub grad requires exactly 2 parent tensors, got {len(args)}\"\n",
    "        \n",
    "        original_a, original_b = args\n",
    "        \n",
    "        # Para subtração: d/da (a - b) = 1, d/db (a - b) = -1\n",
    "        grad_a = Tensor(back_grad._arr.copy(), requires_grad=False, name=\"Sub_grad\")\n",
    "        grad_b = Tensor(-back_grad._arr.copy(), requires_grad=False, name=\"Sub_grad\")\n",
    "        \n",
    "        # Fazer unbroadcast dos gradientes para os shapes originais\n",
    "        grad_a = self._unbroadcast_gradient(grad_a, original_a.shape)\n",
    "        grad_b = self._unbroadcast_gradient(grad_b, original_b.shape)\n",
    "        \n",
    "        # Verificar shapes dos gradientes\n",
    "        assert grad_a.shape == original_a.shape, f\"Gradient shape {grad_a.shape} must match parent shape {original_a.shape}\"\n",
    "        assert grad_b.shape == original_b.shape, f\"Gradient shape {grad_b.shape} must match parent shape {original_b.shape}\"\n",
    "        \n",
    "        return [grad_a, grad_b]\n",
    "    \n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "sub = Sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6f53df08",
   "metadata": {
    "tags": [
     "prod"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Prod(Op):\n",
    "    \"\"\"Prod(a, b): produto element-wise a * b (igual ao Mul)\"\"\"\n",
    "    \n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza o produto element-wise\"\"\"\n",
    "        assert len(args) == 2, f\"Prod operation requires exactly 2 arguments, got {len(args)}\"\n",
    "        \n",
    "        # Converter argumentos para Tensors se necessário\n",
    "        original_a, original_b = self._ensure_tensor(args[0]), self._ensure_tensor(args[1])\n",
    "        \n",
    "        # Fazer broadcasting dos tensors\n",
    "        a, b = self._broadcast_tensors(original_a, original_b)\n",
    "        \n",
    "        # Agora os shapes devem ser iguais\n",
    "        assert a.shape == b.shape, f\"After broadcasting, tensors must have same shape: {a.shape} vs {b.shape}\"\n",
    "        \n",
    "        # Realizar a operação (produto element-wise)\n",
    "        result_arr = a._arr * b._arr\n",
    "        \n",
    "        # Verificar que o resultado tem o shape esperado\n",
    "        assert result_arr.shape == a.shape, f\"Result shape {result_arr.shape} should match input shape {a.shape}\"\n",
    "        \n",
    "        # Criar tensor resultado com referências aos pais ORIGINAIS\n",
    "        parents = [original_a, original_b]\n",
    "        \n",
    "        result = Tensor(\n",
    "            result_arr,\n",
    "            parents=parents,\n",
    "            requires_grad=(original_a.requires_grad or original_b.requires_grad),\n",
    "            operation=self,\n",
    "            name=NameManager.new(\"prod\")\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais\"\"\"\n",
    "        assert len(args) == 2, f\"Prod grad requires exactly 2 parent tensors, got {len(args)}\"\n",
    "        \n",
    "        original_a, original_b = args\n",
    "        \n",
    "        # Fazer broadcasting novamente para calcular gradientes\n",
    "        a, b = self._broadcast_tensors(original_a, original_b)\n",
    "        \n",
    "        # d/da (a * b) = b, d/db (a * b) = a\n",
    "        grad_a = Tensor(back_grad._arr * b._arr, requires_grad=False, name=\"Prod_grad\")\n",
    "        grad_b = Tensor(back_grad._arr * a._arr, requires_grad=False, name=\"Prod_grad\")\n",
    "        \n",
    "        # Fazer unbroadcast dos gradientes para os shapes originais\n",
    "        grad_a = self._unbroadcast_gradient(grad_a, original_a.shape)\n",
    "        grad_b = self._unbroadcast_gradient(grad_b, original_b.shape)\n",
    "        \n",
    "        # Verificar shapes dos gradientes\n",
    "        assert grad_a.shape == original_a.shape, f\"Gradient shape {grad_a.shape} must match parent shape {original_a.shape}\"\n",
    "        assert grad_b.shape == original_b.shape, f\"Gradient shape {grad_b.shape} must match parent shape {original_b.shape}\"\n",
    "        \n",
    "        return [grad_a, grad_b]\n",
    "    \n",
    "# Instancia a operação\n",
    "prod = Prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8f3838a7",
   "metadata": {
    "tags": [
     "sin"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sin(Op):\n",
    "    \"\"\"Sin(a): seno element-wise\"\"\"\n",
    "    \n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza o seno element-wise\"\"\"\n",
    "        assert len(args) == 1, f\"Sin operation requires exactly 1 argument, got {len(args)}\"\n",
    "        \n",
    "        a = self._ensure_tensor(args[0])\n",
    "        \n",
    "        # Calcular seno element-wise\n",
    "        result_arr = np.sin(a._arr)\n",
    "        \n",
    "        # Verificar que o resultado tem o shape esperado\n",
    "        assert result_arr.shape == a.shape, f\"Result shape {result_arr.shape} should match input shape {a.shape}\"\n",
    "        \n",
    "        # Criar tensor resultado\n",
    "        result = Tensor(\n",
    "            result_arr,\n",
    "            parents=[a],\n",
    "            requires_grad=a.requires_grad,\n",
    "            operation=self,\n",
    "            name=NameManager.new(\"sin\")\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais\"\"\"\n",
    "        assert len(args) == 1, f\"Sin grad requires exactly 1 parent tensor, got {len(args)}\"\n",
    "        \n",
    "        a = args[0]\n",
    "        \n",
    "        # Verificar que back_grad tem o shape correto\n",
    "        assert back_grad.shape == a.shape, f\"Back gradient shape {back_grad.shape} must match tensor shape {a.shape}\"\n",
    "        \n",
    "        # Para seno: d/da sin(a) = cos(a)\n",
    "        grad_a = Tensor(back_grad._arr * np.cos(a._arr), requires_grad=False, name=\"Sin_grad\")\n",
    "        \n",
    "        # Verificar shape do gradiente\n",
    "        assert grad_a.shape == a.shape, f\"Gradient shape {grad_a.shape} must match parent shape {a.shape}\"\n",
    "        \n",
    "        return [grad_a]\n",
    "    \n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "sin = Sin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "138cb8ef",
   "metadata": {
    "tags": [
     "cos"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Cos(Op):\n",
    "    \"\"\"Cos(a): cosseno element-wise\"\"\"\n",
    "    \n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza o cosseno element-wise\"\"\"\n",
    "        assert len(args) == 1, f\"Cos operation requires exactly 1 argument, got {len(args)}\"\n",
    "        \n",
    "        a = self._ensure_tensor(args[0])\n",
    "        \n",
    "        # Calcular cosseno element-wise\n",
    "        result_arr = np.cos(a._arr)\n",
    "        \n",
    "        # Verificar que o resultado tem o shape esperado\n",
    "        assert result_arr.shape == a.shape, f\"Result shape {result_arr.shape} should match input shape {a.shape}\"\n",
    "        \n",
    "        # Criar tensor resultado\n",
    "        result = Tensor(\n",
    "            result_arr,\n",
    "            parents=[a],\n",
    "            requires_grad=a.requires_grad,\n",
    "            operation=self,\n",
    "            name=NameManager.new(\"cos\")\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais\"\"\"\n",
    "        assert len(args) == 1, f\"Cos grad requires exactly 1 parent tensor, got {len(args)}\"\n",
    "        \n",
    "        a = args[0]\n",
    "        \n",
    "        # Verificar que back_grad tem o shape correto\n",
    "        assert back_grad.shape == a.shape, f\"Back gradient shape {back_grad.shape} must match tensor shape {a.shape}\"\n",
    "        \n",
    "        # Para cosseno: d/da cos(a) = -sin(a)\n",
    "        grad_a = Tensor(back_grad._arr * (-np.sin(a._arr)), requires_grad=False, name=\"Cos_grad\")\n",
    "        \n",
    "        # Verificar shape do gradiente\n",
    "        assert grad_a.shape == a.shape, f\"Gradient shape {grad_a.shape} must match parent shape {a.shape}\"\n",
    "        \n",
    "        return [grad_a]\n",
    "    \n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "cos = Cos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "46eac52c",
   "metadata": {
    "tags": [
     "sum"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sum(Op):\n",
    "    \"\"\"Sum(a): soma todos os elementos do tensor\"\"\"\n",
    "    \n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a soma de todos os elementos\"\"\"\n",
    "        assert len(args) == 1, f\"Sum operation requires exactly 1 argument, got {len(args)}\"\n",
    "        \n",
    "        a = self._ensure_tensor(args[0])\n",
    "        \n",
    "        # Somar todos os elementos\n",
    "        result_arr = np.array([[np.sum(a._arr)]])\n",
    "        \n",
    "        # Criar tensor resultado (sempre shape (1,1))\n",
    "        result = Tensor(\n",
    "            result_arr,\n",
    "            parents=[a],\n",
    "            requires_grad=a.requires_grad,\n",
    "            operation=self,\n",
    "            name=NameManager.new(\"sum\")\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais\"\"\"\n",
    "        assert len(args) == 1, f\"Sum grad requires exactly 1 parent tensor, got {len(args)}\"\n",
    "        \n",
    "        a = args[0]\n",
    "        \n",
    "        # Verificar que back_grad é escalar\n",
    "        assert back_grad.shape == (1, 1), f\"Sum back gradient must be scalar, got shape {back_grad.shape}\"\n",
    "        \n",
    "        # Para soma, o gradiente é o mesmo valor para todos os elementos\n",
    "        grad_a = Tensor(np.full(a.shape, back_grad._arr[0, 0]), requires_grad=False, name=\"Sum_grad\")\n",
    "        \n",
    "        # Verificar shape do gradiente\n",
    "        assert grad_a.shape == a.shape, f\"Gradient shape {grad_a.shape} must match parent shape {a.shape}\"\n",
    "        \n",
    "        return [grad_a]\n",
    "    \n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "# ⚠️ vamos chamar de my_sum porque python ja possui uma funcao sum\n",
    "my_sum = Sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e098a39c",
   "metadata": {
    "tags": [
     "mean"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Mean(Op):\n",
    "    \"\"\"Mean(a): média de todos os elementos do tensor\"\"\"\n",
    "    \n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a média de todos os elementos\"\"\"\n",
    "        assert len(args) == 1, f\"Mean operation requires exactly 1 argument, got {len(args)}\"\n",
    "        \n",
    "        a = self._ensure_tensor(args[0])\n",
    "        \n",
    "        # Calcular média de todos os elementos\n",
    "        result_arr = np.array([[np.mean(a._arr)]])\n",
    "        \n",
    "        # Criar tensor resultado (sempre shape (1,1))\n",
    "        result = Tensor(\n",
    "            result_arr,\n",
    "            parents=[a],\n",
    "            requires_grad=a.requires_grad,\n",
    "            operation=self,\n",
    "            name=NameManager.new(\"mean\")\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais\"\"\"\n",
    "        assert len(args) == 1, f\"Mean grad requires exactly 1 parent tensor, got {len(args)}\"\n",
    "        \n",
    "        a = args[0]\n",
    "        \n",
    "        # Verificar que back_grad é escalar\n",
    "        assert back_grad.shape == (1, 1), f\"Mean back gradient must be scalar, got shape {back_grad.shape}\"\n",
    "        \n",
    "        # Para média, o gradiente é 1/n para cada elemento\n",
    "        n = a.shape[0] * a.shape[1]  # número total de elementos\n",
    "        grad_value = back_grad._arr[0, 0] / n\n",
    "        grad_a = Tensor(np.full(a.shape, grad_value), requires_grad=False, name=\"Mean_grad\")\n",
    "        \n",
    "        # Verificar shape do gradiente\n",
    "        assert grad_a.shape == a.shape, f\"Gradient shape {grad_a.shape} must match parent shape {a.shape}\"\n",
    "        \n",
    "        return [grad_a]\n",
    "    \n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "mean = Mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "37692879",
   "metadata": {
    "tags": [
     "square"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Square(Op):\n",
    "    \"\"\"Square(a): a^2 element-wise\"\"\"\n",
    "    \n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza o quadrado element-wise\"\"\"\n",
    "        assert len(args) == 1, f\"Square operation requires exactly 1 argument, got {len(args)}\"\n",
    "        \n",
    "        a = self._ensure_tensor(args[0])\n",
    "        \n",
    "        # Elevar ao quadrado element-wise\n",
    "        result_arr = a._arr ** 2\n",
    "        \n",
    "        # Verificar que o resultado tem o shape esperado\n",
    "        assert result_arr.shape == a.shape, f\"Result shape {result_arr.shape} should match input shape {a.shape}\"\n",
    "        \n",
    "        # Criar tensor resultado\n",
    "        result = Tensor(\n",
    "            result_arr,\n",
    "            parents=[a],\n",
    "            requires_grad=a.requires_grad,\n",
    "            operation=self,\n",
    "            name=NameManager.new(\"square\")\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais\"\"\"\n",
    "        assert len(args) == 1, f\"Square grad requires exactly 1 parent tensor, got {len(args)}\"\n",
    "        \n",
    "        a = args[0]\n",
    "        \n",
    "        # Verificar que back_grad tem o shape correto\n",
    "        assert back_grad.shape == a.shape, f\"Back gradient shape {back_grad.shape} must match tensor shape {a.shape}\"\n",
    "        \n",
    "        # Para quadrado: d/da (a^2) = 2a\n",
    "        grad_a = Tensor(back_grad._arr * 2 * a._arr, requires_grad=False, name=\"Square_grad\")\n",
    "        \n",
    "        # Verificar shape do gradiente\n",
    "        assert grad_a.shape == a.shape, f\"Gradient shape {grad_a.shape} must match parent shape {a.shape}\"\n",
    "        \n",
    "        return [grad_a]\n",
    "    \n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "square = Square()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6542807d",
   "metadata": {
    "tags": [
     "matmul"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class MatMul(Op):\n",
    "    \"\"\"MatMul(a, b): a @ b (matrix multiplication)\"\"\"\n",
    "    \n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a multiplicação matricial\"\"\"\n",
    "        assert len(args) == 2, f\"MatMul operation requires exactly 2 arguments, got {len(args)}\"\n",
    "        \n",
    "        # Converter argumentos para Tensors se necessário\n",
    "        a, b = self._ensure_tensor(args[0]), self._ensure_tensor(args[1])\n",
    "        \n",
    "        # Verificar compatibilidade de dimensões para multiplicação matricial\n",
    "        assert len(a.shape) == 2 and len(b.shape) == 2, f\"MatMul requires 2D tensors, got shapes {a.shape} and {b.shape}\"\n",
    "        assert a.shape[1] == b.shape[0], f\"Incompatible shapes for matrix multiplication: {a.shape} and {b.shape}\"\n",
    "        \n",
    "        # Realizar a operação\n",
    "        result_arr = a._arr @ b._arr\n",
    "        \n",
    "        # Verificar que o resultado tem o shape esperado\n",
    "        expected_shape = (a.shape[0], b.shape[1])\n",
    "        assert result_arr.shape == expected_shape, f\"Result shape {result_arr.shape} should be {expected_shape}\"\n",
    "        \n",
    "        # Criar tensor resultado\n",
    "        parents = [a, b]\n",
    "        \n",
    "        result = Tensor(\n",
    "            result_arr,\n",
    "            parents=parents,\n",
    "            requires_grad=(a.requires_grad or b.requires_grad),\n",
    "            operation=self,\n",
    "            name=NameManager.new(\"matmul\")\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais\"\"\"\n",
    "        assert len(args) == 2, f\"MatMul grad requires exactly 2 parent tensors, got {len(args)}\"\n",
    "        \n",
    "        a, b = args\n",
    "        \n",
    "        # Verificar que back_grad tem o shape correto (resultado da multiplicação)\n",
    "        expected_grad_shape = (a.shape[0], b.shape[1])\n",
    "        assert back_grad.shape == expected_grad_shape, f\"Back gradient shape {back_grad.shape} must match result shape {expected_grad_shape}\"\n",
    "        \n",
    "        # d/da (a @ b) = back_grad @ b.T\n",
    "        # d/db (a @ b) = a.T @ back_grad\n",
    "        grad_a = Tensor(back_grad._arr @ b._arr.T, requires_grad=False, name=\"matmul_grad\")\n",
    "        grad_b = Tensor(a._arr.T @ back_grad._arr, requires_grad=False, name=\"matmul_grad\")\n",
    "        \n",
    "        # Verificar shapes dos gradientes\n",
    "        assert grad_a.shape == a.shape, f\"Gradient a shape {grad_a.shape} must match parent shape {a.shape}\"\n",
    "        assert grad_b.shape == b.shape, f\"Gradient b shape {grad_b.shape} must match parent shape {b.shape}\"\n",
    "        \n",
    "        return [grad_a, grad_b]\n",
    "    \n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "matmul = MatMul()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c08a38e3",
   "metadata": {
    "tags": [
     "exp"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Exp(Op):\n",
    "    \"\"\"Exp(a): exponencial element-wise\"\"\"\n",
    "    \n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a exponencial element-wise\"\"\"\n",
    "        assert len(args) == 1, f\"Exp operation requires exactly 1 argument, got {len(args)}\"\n",
    "        \n",
    "        a = self._ensure_tensor(args[0])\n",
    "        \n",
    "        # Calcular exponencial element-wise\n",
    "        result_arr = np.exp(a._arr)\n",
    "        \n",
    "        # Verificar que o resultado tem o shape esperado\n",
    "        assert result_arr.shape == a.shape, f\"Result shape {result_arr.shape} should match input shape {a.shape}\"\n",
    "        \n",
    "        # Criar tensor resultado\n",
    "        result = Tensor(\n",
    "            result_arr,\n",
    "            parents=[a],\n",
    "            requires_grad=a.requires_grad,\n",
    "            operation=self,\n",
    "            name=NameManager.new(\"exp\")\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais\"\"\"\n",
    "        assert len(args) == 1, f\"Exp grad requires exactly 1 parent tensor, got {len(args)}\"\n",
    "        \n",
    "        a = args[0]\n",
    "        \n",
    "        # Verificar que back_grad tem o shape correto\n",
    "        assert back_grad.shape == a.shape, f\"Back gradient shape {back_grad.shape} must match tensor shape {a.shape}\"\n",
    "        \n",
    "        # Para exponencial: d/da exp(a) = exp(a)\n",
    "        grad_a = Tensor(back_grad._arr * np.exp(a._arr), requires_grad=False, name=\"Exp_grad\")\n",
    "        \n",
    "        # Verificar shape do gradiente\n",
    "        assert grad_a.shape == a.shape, f\"Gradient shape {grad_a.shape} must match parent shape {a.shape}\"\n",
    "        \n",
    "        return [grad_a]\n",
    "    \n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "exp = Exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1acc813e",
   "metadata": {
    "tags": [
     "relu"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class ReLU(Op):\n",
    "    \"\"\"Relu(a): ReLU element-wise\"\"\"\n",
    "    \n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza ReLU element-wise\"\"\"\n",
    "        assert len(args) == 1, f\"Relu operation requires exactly 1 argument, got {len(args)}\"\n",
    "        \n",
    "        a = self._ensure_tensor(args[0])\n",
    "        \n",
    "        # Calcular ReLU element-wise: max(0, a)\n",
    "        result_arr = np.maximum(0.0, a._arr)\n",
    "        \n",
    "        # Verificar que o resultado tem o shape esperado\n",
    "        assert result_arr.shape == a.shape, f\"Result shape {result_arr.shape} should match input shape {a.shape}\"\n",
    "        \n",
    "        # Criar tensor resultado\n",
    "        result = Tensor(\n",
    "            result_arr,\n",
    "            parents=[a],\n",
    "            requires_grad=a.requires_grad,\n",
    "            operation=self,\n",
    "            name=NameManager.new(\"relu\")\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais\"\"\"\n",
    "        assert len(args) == 1, f\"Relu grad requires exactly 1 parent tensor, got {len(args)}\"\n",
    "        \n",
    "        a = args[0]\n",
    "        \n",
    "        # Verificar que back_grad tem o shape correto\n",
    "        assert back_grad.shape == a.shape, f\"Back gradient shape {back_grad.shape} must match tensor shape {a.shape}\"\n",
    "        \n",
    "        # Para ReLU: d/da ReLU(a) = 1 se a > 0, senão 0\n",
    "        relu_grad = np.where(a._arr > 0, 1.0, 0.0)\n",
    "        grad_a = Tensor(back_grad._arr * relu_grad, requires_grad=False, name=\"Relu_grad\")\n",
    "        \n",
    "        # Verificar shape do gradiente\n",
    "        assert grad_a.shape == a.shape, f\"Gradient shape {grad_a.shape} must match parent shape {a.shape}\"\n",
    "        \n",
    "        return [grad_a]\n",
    "    \n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "relu = ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ae499275",
   "metadata": {
    "tags": [
     "sigmoid"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sigmoid(Op):\n",
    "    \"\"\"Sigmoid(a): função sigmoide element-wise\"\"\"\n",
    "    \n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza sigmoide element-wise\"\"\"\n",
    "        assert len(args) == 1, f\"Sigmoid operation requires exactly 1 argument, got {len(args)}\"\n",
    "        \n",
    "        a = self._ensure_tensor(args[0])\n",
    "        \n",
    "        # Calcular sigmoide element-wise: 1 / (1 + exp(-a))\n",
    "        # Usar clip para evitar overflow\n",
    "        clipped = np.clip(a._arr, -500, 500)\n",
    "        result_arr = 1.0 / (1.0 + np.exp(-clipped))\n",
    "        \n",
    "        # Verificar que o resultado tem o shape esperado\n",
    "        assert result_arr.shape == a.shape, f\"Result shape {result_arr.shape} should match input shape {a.shape}\"\n",
    "        \n",
    "        # Criar tensor resultado\n",
    "        result = Tensor(\n",
    "            result_arr,\n",
    "            parents=[a],\n",
    "            requires_grad=a.requires_grad,\n",
    "            operation=self,\n",
    "            name=NameManager.new(\"sigmoid\")\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais\"\"\"\n",
    "        assert len(args) == 1, f\"Sigmoid grad requires exactly 1 parent tensor, got {len(args)}\"\n",
    "        \n",
    "        a = args[0]\n",
    "        \n",
    "        # Verificar que back_grad tem o shape correto\n",
    "        assert back_grad.shape == a.shape, f\"Back gradient shape {back_grad.shape} must match tensor shape {a.shape}\"\n",
    "        \n",
    "        # Para sigmoide: d/da sigmoid(a) = sigmoid(a) * (1 - sigmoid(a))\n",
    "        clipped = np.clip(a._arr, -500, 500)\n",
    "        sigmoid_val = 1.0 / (1.0 + np.exp(-clipped))\n",
    "        sigmoid_grad = sigmoid_val * (1.0 - sigmoid_val)\n",
    "        grad_a = Tensor(back_grad._arr * sigmoid_grad, requires_grad=False, name=\"Sigmoid_grad\")\n",
    "        \n",
    "        # Verificar shape do gradiente\n",
    "        assert grad_a.shape == a.shape, f\"Gradient shape {grad_a.shape} must match parent shape {a.shape}\"\n",
    "        \n",
    "        return [grad_a]\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "sigmoid = Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6ce464ba",
   "metadata": {
    "tags": [
     "tanh"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Tanh(Op):\n",
    "    \"\"\"Tanh(a): tangente hiperbólica element-wise\"\"\"\n",
    "    \n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza tanh element-wise\"\"\"\n",
    "        assert len(args) == 1, f\"Tanh operation requires exactly 1 argument, got {len(args)}\"\n",
    "        \n",
    "        a = self._ensure_tensor(args[0])\n",
    "        \n",
    "        # Calcular tanh element-wise\n",
    "        result_arr = np.tanh(a._arr)\n",
    "        \n",
    "        # Verificar que o resultado tem o shape esperado\n",
    "        assert result_arr.shape == a.shape, f\"Result shape {result_arr.shape} should match input shape {a.shape}\"\n",
    "        \n",
    "        # Criar tensor resultado\n",
    "        result = Tensor(\n",
    "            result_arr,\n",
    "            parents=[a],\n",
    "            requires_grad=a.requires_grad,\n",
    "            operation=self,\n",
    "            name=NameManager.new(\"tanh\")\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais\"\"\"\n",
    "        assert len(args) == 1, f\"Tanh grad requires exactly 1 parent tensor, got {len(args)}\"\n",
    "        \n",
    "        a = args[0]\n",
    "        \n",
    "        # Verificar que back_grad tem o shape correto\n",
    "        assert back_grad.shape == a.shape, f\"Back gradient shape {back_grad.shape} must match tensor shape {a.shape}\"\n",
    "        \n",
    "        # Para tanh: d/da tanh(a) = 1 - tanh²(a)\n",
    "        tanh_val = np.tanh(a._arr)\n",
    "        tanh_grad = 1.0 - tanh_val**2\n",
    "        grad_a = Tensor(back_grad._arr * tanh_grad, requires_grad=False, name=\"Tanh_grad\")\n",
    "        \n",
    "        # Verificar shape do gradiente\n",
    "        assert grad_a.shape == a.shape, f\"Gradient shape {grad_a.shape} must match parent shape {a.shape}\"\n",
    "        \n",
    "        return [grad_a]\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "tanh = Tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f15a37eb",
   "metadata": {
    "tags": [
     "softmax"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Softmax(Op):\n",
    "    \"\"\"Softmax(a): função softmax aplicada na dimensão apropriada\"\"\"\n",
    "    \n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza softmax na dimensão apropriada\"\"\"\n",
    "        assert len(args) == 1, f\"Softmax operation requires exactly 1 argument, got {len(args)}\"\n",
    "        \n",
    "        a = self._ensure_tensor(args[0])\n",
    "        \n",
    "        # Determinar o eixo correto para softmax\n",
    "        # Se é um vetor coluna (n, 1), aplicar softmax ao longo do eixo 0\n",
    "        # Se é um vetor linha (1, n), aplicar softmax ao longo do eixo 1\n",
    "        # Se é uma matriz (m, n) com m > 1 e n > 1, aplicar ao longo do eixo 1 (features)\n",
    "        \n",
    "        if a.shape[1] == 1:  # Vetor coluna (n, 1)\n",
    "            axis = 0\n",
    "        else:  # Matriz geral ou vetor linha, aplicar ao longo do eixo 1\n",
    "            axis = 1\n",
    "        \n",
    "        # Calcular softmax ao longo do eixo determinado\n",
    "        # Subtrair o máximo para estabilidade numérica\n",
    "        max_vals = np.max(a._arr, axis=axis, keepdims=True)\n",
    "        exp_vals = np.exp(a._arr - max_vals)\n",
    "        sum_exp = np.sum(exp_vals, axis=axis, keepdims=True)\n",
    "        result_arr = exp_vals / sum_exp\n",
    "        \n",
    "        # Verificar que o resultado tem o shape esperado\n",
    "        assert result_arr.shape == a.shape, f\"Result shape {result_arr.shape} should match input shape {a.shape}\"\n",
    "        \n",
    "        # Criar tensor resultado\n",
    "        result = Tensor(\n",
    "            result_arr,\n",
    "            parents=[a],\n",
    "            requires_grad=a.requires_grad,\n",
    "            operation=self,\n",
    "            name=NameManager.new(\"softmax\")\n",
    "        )\n",
    "        \n",
    "        # Guardar informações para o gradiente\n",
    "        result._softmax_axis = axis\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais\"\"\"\n",
    "        assert len(args) == 1, f\"Softmax grad requires exactly 1 parent tensor, got {len(args)}\"\n",
    "        \n",
    "        a = args[0]\n",
    "        \n",
    "        # Verificar que back_grad tem o shape correto\n",
    "        assert back_grad.shape == a.shape, f\"Back gradient shape {back_grad.shape} must match tensor shape {a.shape}\"\n",
    "        \n",
    "        # Determinar o eixo (mesmo critério do forward)\n",
    "        if a.shape[1] == 1:  # Vetor coluna (n, 1)\n",
    "            axis = 0\n",
    "        else:  # Matriz geral ou vetor linha\n",
    "            axis = 1\n",
    "        \n",
    "        # Calcular softmax novamente para obter os valores\n",
    "        max_vals = np.max(a._arr, axis=axis, keepdims=True)\n",
    "        exp_vals = np.exp(a._arr - max_vals)\n",
    "        sum_exp = np.sum(exp_vals, axis=axis, keepdims=True)\n",
    "        softmax_vals = exp_vals / sum_exp\n",
    "        \n",
    "        # Para softmax: jacobiano é softmax[i] * (δ[i,j] - softmax[j])\n",
    "        # Implementação eficiente: s * (back_grad - (s * back_grad).sum(axis, keepdims=True))\n",
    "        grad_arr = softmax_vals * (back_grad._arr - np.sum(softmax_vals * back_grad._arr, axis=axis, keepdims=True))\n",
    "        grad_a = Tensor(grad_arr, requires_grad=False, name=\"Softmax_grad\")\n",
    "        \n",
    "        # Verificar shape do gradiente\n",
    "        assert grad_a.shape == a.shape, f\"Gradient shape {grad_a.shape} must match parent shape {a.shape}\"\n",
    "        \n",
    "        return [grad_a]\n",
    "    \n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "softmax = Softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12256fd7",
   "metadata": {},
   "source": [
    "\n",
    "### ‼️ Regras e Pontos de Atenção‼️\n",
    "\n",
    "- Vamos fazer a hipótese simplificadora que Tensores devem ser sempre matrizes. Por exemplo, o escalar 2 deve ser armazado em `_arr` como a matriz `[[2]]`. De forma similar, a lista `[1, 2, 3]` deve ser armazenada em `_arr` como em uma matriz coluna.\n",
    "\n",
    "- Devem ser realizados `asserts` nas operações para garantir que os shapes dos operandos fazem sentido. Esta verificação também deve ser feita depois das operações que manipulam gradientes de tensores.\n",
    "\n",
    "- Devem ser respeitados os nomes dos atributos, métodos e classes para viabilizar os testes automáticos.\n",
    "\n",
    "- Gradientes devem ser calculados usando uma passada pelo grafo computacional.\n",
    "\n",
    "- Os gradientes devem ser somados e não substituídos nas chamadas de  backward. Isto vai permitir que os gradientes sejam acumulados entre amostras do dataset e que os resultados sejam corretos mesmo em caso de ramificações e junções no grafo computacional.\n",
    "\n",
    "- Lembre-se de zerar os gradientes após cada passo de gradient descent (atualização dos parâmetros).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc927248",
   "metadata": {},
   "source": [
    "## Testes Básicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae08a8",
   "metadata": {},
   "source": [
    "Estes testes avaliam se a derivada da função está sendo calculada corretamente, mas em muitos casos **não** avaliam se os gradientes backpropagados estão sendo incorporados corretamente. Esta avaliação será feita nos problemas da próxima seção."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05318a9",
   "metadata": {},
   "source": [
    "Operador de Soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9fd20550",
   "metadata": {
    "tags": [
     "test_add"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[1.]\n",
      " [1.]\n",
      " [1.]], name=in:9, shape=(3, 1))\n",
      "Tensor([[1.]\n",
      " [1.]\n",
      " [1.]], name=in:10, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# add\n",
    "\n",
    "a = Tensor([1.0, 2.0, 3.0])\n",
    "b = Tensor([4.0, 5.0, 6.0])\n",
    "c = add(a, b)\n",
    "d = add(c, 3.0)\n",
    "d.backward()\n",
    "\n",
    "# esperado: matrizes coluna contendo 1\n",
    "print(a.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac72b1a",
   "metadata": {},
   "source": [
    "Operador de Subtração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "612377aa",
   "metadata": {
    "tags": [
     "test_sub"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[1.]\n",
      " [1.]\n",
      " [1.]], name=in:19, shape=(3, 1))\n",
      "Tensor([[-1.]\n",
      " [-1.]\n",
      " [-1.]], name=in:20, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# sub\n",
    "\n",
    "a = Tensor([1.0, 2.0, 3.0])\n",
    "b = Tensor([4.0, 5.0, 6.0])\n",
    "c = sub(a, b)\n",
    "d = sub(c, 3.0)\n",
    "d.backward()\n",
    "\n",
    "# esperado: matrizes coluna contendo 1 e -1\n",
    "print(a.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7c8e63",
   "metadata": {},
   "source": [
    "Operador de Produto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dc60de82",
   "metadata": {
    "tags": [
     "test_prod"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[12.]\n",
      " [15.]\n",
      " [18.]], name=in:30, shape=(3, 1))\n",
      "Tensor([[3.]\n",
      " [6.]\n",
      " [9.]], name=in:31, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# prod\n",
    "\n",
    "a = Tensor([1.0, 2.0, 3.0])\n",
    "b = Tensor([4.0, 5.0, 6.0])\n",
    "c = prod(a, b)\n",
    "d = prod(c, 3.0)\n",
    "d.backward()\n",
    "\n",
    "# esperado: [12, 15, 18]^T\n",
    "print(a.grad)\n",
    "# esperado: [3, 6, 9]^T\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d91e1c3",
   "metadata": {},
   "source": [
    "Operadores trigonométricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6185a989",
   "metadata": {
    "tags": [
     "test_sin_cos"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[-1.]\n",
      " [ 1.]\n",
      " [-1.]], name=in:39, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# sin e cos\n",
    "\n",
    "a = Tensor([np.pi, 0, np.pi/2])\n",
    "b = sin(a)\n",
    "c = cos(a)\n",
    "d = my_sum(add(b, c))\n",
    "d.backward()\n",
    "\n",
    "# esperado: [-1, 1, -1]^T\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5f29f232",
   "metadata": {
    "tags": [
     "test_sum"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]], name=in:50, shape=(4, 1))\n",
      "\n",
      "\n",
      "a.grad = Tensor([[4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]], name=in:50, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Sum\n",
    "\n",
    "a = Tensor([3.0, 1.0, 0.0, 2.0])\n",
    "b = add(prod(a, 3.0), a)\n",
    "c = my_sum(b)\n",
    "c.backward()\n",
    "\n",
    "# esperado: [4, 4, 4, 4]^T\n",
    "print(a.grad)\n",
    "\n",
    "print(\"\\n\\na.grad =\", a.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8943e71a",
   "metadata": {
    "tags": [
     "test_mean"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[0.25]\n",
      " [0.25]\n",
      " [0.25]\n",
      " [0.25]], name=in:54, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Mean\n",
    "\n",
    "a = Tensor([3.0, 1.0, 0.0, 2.0])\n",
    "b = mean(a)\n",
    "b.backward()\n",
    "\n",
    "# esperado: [0.25, 0.25, 0.25, 0.25]^T\n",
    "print(a.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1c7dbd2c",
   "metadata": {
    "tags": [
     "test_square"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[9.]\n",
      " [1.]\n",
      " [0.]\n",
      " [4.]], name=square:0, shape=(4, 1))\n",
      "Tensor([[6.]\n",
      " [2.]\n",
      " [0.]\n",
      " [4.]], name=in:58, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Square\n",
    "\n",
    "a = Tensor([3.0, 1.0, 0.0, 2.0])\n",
    "b = square(a)\n",
    "\n",
    "# esperado: [9, 1, 0, 4]^T\n",
    "print(b)\n",
    "\n",
    "b.backward()\n",
    "\n",
    "# esperado: [6, 2, 0, 4]\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "02f2ead7",
   "metadata": {
    "tags": [
     "test_matmul"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[14.]\n",
      " [32.]\n",
      " [50.]], name=matmul:0, shape=(3, 1))\n",
      "Tensor([[1. 2. 3.]\n",
      " [1. 2. 3.]\n",
      " [1. 2. 3.]], name=in:63, shape=(3, 3))\n",
      "Tensor([[12.]\n",
      " [15.]\n",
      " [18.]], name=in:64, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# matmul\n",
    "\n",
    "W = Tensor([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0],\n",
    "    [7.0, 8.0, 9.0]\n",
    "])\n",
    "\n",
    "v = Tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "z = matmul(W, v)\n",
    "\n",
    "# esperado: [14, 32, 50]^T\n",
    "print(z)\n",
    "\n",
    "z.backward()\n",
    "\n",
    "# esperado:\n",
    "# [1, 2, 3]\n",
    "# [1, 2, 3]\n",
    "# [1, 2, 3]\n",
    "print(W.grad)\n",
    "\n",
    "# esperado: [12, 15, 18]^T\n",
    "print(v.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "706212d2",
   "metadata": {
    "tags": [
     "test_exp",
     "text_exp"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[ 2.71828183]\n",
      " [ 7.3890561 ]\n",
      " [20.08553692]], name=exp:0, shape=(3, 1))\n",
      "Tensor([[ 2.71828183]\n",
      " [ 7.3890561 ]\n",
      " [20.08553692]], name=in:68, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# Exp\n",
    "\n",
    "v = Tensor([1.0, 2.0, 3.0])\n",
    "w = exp(v)\n",
    "\n",
    "# esperado: [2.718..., 7.389..., 20.085...]^T\n",
    "print(w)\n",
    "\n",
    "w.backward()\n",
    "\n",
    "# esperado: [2.718..., 7.389..., 20.085...]^T\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9510d010",
   "metadata": {
    "tags": [
     "test_relu"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [3.]], name=relu:0, shape=(4, 1))\n",
      "Tensor([[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]], name=in:72, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Relu\n",
    "\n",
    "v = Tensor([-1.0, 0.0, 1.0, 3.0])\n",
    "w = relu(v)\n",
    "\n",
    "# esperado: [0, 0, 1, 3]^T\n",
    "print(w)\n",
    "\n",
    "w.backward()\n",
    "\n",
    "# esperado: [0, 0, 1, 1]^T\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2f0fbf8d",
   "metadata": {
    "tags": [
     "test_sigmoid"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[0.26894142]\n",
      " [0.5       ]\n",
      " [0.73105858]\n",
      " [0.95257413]], name=sigmoid:0, shape=(4, 1))\n",
      "Tensor([[0.19661193]\n",
      " [0.25      ]\n",
      " [0.19661193]\n",
      " [0.04517666]], name=in:76, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid\n",
    "\n",
    "v = Tensor([-1.0, 0.0, 1.0, 3.0])\n",
    "w = sigmoid(v)\n",
    "\n",
    "# esperado: [0.268.., 0.5, 0.731.., 0.952..]^T\n",
    "print(w)\n",
    "\n",
    "w.backward()\n",
    "\n",
    "# esperado: [0.196..., 0.25, 0.196..., 0.045...]^T\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7e867dec",
   "metadata": {
    "tags": [
     "test_tanh"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[-0.76159416]\n",
      " [ 0.        ]\n",
      " [ 0.76159416]\n",
      " [ 0.99505475]], name=tanh:0, shape=(4, 1))\n",
      "Tensor([[0.41997434]\n",
      " [1.        ]\n",
      " [0.41997434]\n",
      " [0.00986604]], name=in:80, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Tanh\n",
    "\n",
    "v = Tensor([-1.0, 0.0, 1.0, 3.0])\n",
    "w = tanh(v)\n",
    "\n",
    "# esperado: [[-0.76159416, 0., 0.76159416, 0.99505475]^T\n",
    "print(w)\n",
    "\n",
    "w.backward()\n",
    "\n",
    "# esperado: [0.41997434, 1., 0.41997434, 0.00986604]^T\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7fd3235d",
   "metadata": {
    "tags": [
     "test_softmax"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[0.00381737]\n",
      " [0.13970902]\n",
      " [0.23034123]\n",
      " [0.62613238]], name=softmax:0, shape=(4, 1))\n",
      "MSE: Tensor([[0.36424932]], name=mean:1, shape=(1, 1))\n",
      "Tensor([[-0.00278095]\n",
      " [-0.02243068]\n",
      " [-0.02654377]\n",
      " [ 0.05175539]], name=in:88, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Softmax\n",
    "\n",
    "x = Tensor([-3.1, 0.5, 1.0, 2.0])\n",
    "y = softmax(x)\n",
    "\n",
    "# esperado: [0.00381737, 0.13970902, 0.23034123, 0.62613238]^T\n",
    "print(y)\n",
    "\n",
    "# como exemplo, calcula o MSE para um target vector\n",
    "diff = sub(y, [1, 0, 0, 0])\n",
    "sq = square(diff)\n",
    "a = mean(sq)\n",
    "\n",
    "# esperado: 0.36424932\n",
    "print(\"MSE:\", a)\n",
    "\n",
    "a.backward()\n",
    "\n",
    "# esperado: [-0.00278095, -0.02243068, -0.02654377, 0.05175539]^T\n",
    "print(x.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27a3d9f",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "### Principais\n",
    "\n",
    "- [Build your own pytorch](https://www.peterholderrieth.com/blog/2023/Build-Your-Own-Pytorch-1-Computation-Graphs/)\n",
    "- [Build your own Pytorch - 2: Backpropagation](https://www.peterholderrieth.com/blog/2023/Build-Your-Own-Pytorch-2-Autograd/)\n",
    "- [Build your own PyTorch - 3: Training a Neural Network with self-made AD software](https://www.peterholderrieth.com/blog/2023/Build-Your-Own-Pytorch-3-Build-Classifier/)\n",
    "- [Pytorch: A Gentle Introduction to torch.autograd](https://docs.pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
    "- [Automatic Differentiation with torch.autograd](https://docs.pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)\n",
    "\n",
    "### Secundárias\n",
    "\n",
    "- [Tom Roth: Building a computational graph: part 1](https://tomroth.dev/compgraph1/)\n",
    "- [Tom Roth: Building a computational graph: part 2](https://tomroth.dev/compgraph2/)\n",
    "- [Tom Roth: Building a computational graph: part 3](https://tomroth.dev/compgraph3/)\n",
    "- [Roger Grosse (Toronto) class on Automatic Differentiation](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf)\n",
    "- [Computational graphs and gradient flows](https://simple-english-machine-learning.readthedocs.io/en/latest/neural-networks/computational-graphs.html)\n",
    "- [Colah Visual Blog: Backprop](https://colah.github.io/posts/2015-08-Backprop/)\n",
    "- [Towards Data Science: Automatic Differentiation (AutoDiff): A Brief Intro with Examples](https://towardsdatascience.com/automatic-differentiation-autodiff-a-brief-intro-with-examples-3f3d257ffe3b/)\n",
    "- [A Hands-on Introduction to Automatic Differentiation - Part 1](https://mostafa-samir.github.io/auto-diff-pt1/)\n",
    "- [Build Your own Deep Learning Framework - A Hands-on Introduction to Automatic Differentiation - Part 2](https://mostafa-samir.github.io/auto-diff-pt1/)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
